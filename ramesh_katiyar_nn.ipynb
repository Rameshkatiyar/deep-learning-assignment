{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Detect Melanoma\n",
        "**Problem statement:** To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis."
      ],
      "metadata": {
        "id": "eRHoxVprIL2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z113XxIlAmwg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import Libraries"
      ],
      "metadata": {
        "id": "pTNyjoEgvMJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from glob import glob\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D"
      ],
      "metadata": {
        "id": "W0ULkEHgDqZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path=\"/content/gdrive/MyDrive/PG_AI_ML/TestData/skin-cancer-data/Train\"\n",
        "test_path=\"/content/gdrive/MyDrive/PG_AI_ML/TestData/skin-cancer-data/Test\"\n",
        "\n",
        "data_dir_train = pathlib.Path(train_path)\n",
        "data_dir_test = pathlib.Path(test_path)"
      ],
      "metadata": {
        "id": "xEPNSLx2Dqcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n",
        "print(image_count_train)\n",
        "image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n",
        "print(image_count_test)"
      ],
      "metadata": {
        "id": "BdCirATGDw1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Preparation\n"
      ],
      "metadata": {
        "id": "9edrTe5eFZMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ],
      "metadata": {
        "id": "5vlI1L9bDw4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Data Set Creation\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir_train, labels='inferred', label_mode='categorical',\n",
        "    class_names=None, color_mode='rgb', batch_size=32, image_size=(180,\n",
        "    180), shuffle=True, seed=123, validation_split=0.2, subset='training',\n",
        "    interpolation='bilinear', follow_links=False, smart_resize=False\n",
        ")\n",
        "  "
      ],
      "metadata": {
        "id": "Xm6uhkAfDw7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation Data Set Creation\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir_train, labels='inferred', label_mode='categorical',\n",
        "    class_names=None, color_mode='rgb', batch_size=32, image_size=(180,\n",
        "    180), shuffle=True, seed=123, validation_split=0.2, subset='validation',\n",
        "    interpolation='bilinear', follow_links=False, smart_resize=False\n",
        ")"
      ],
      "metadata": {
        "id": "OU3h_F5dDw94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "CbeiHJfADxDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Visualize the data"
      ],
      "metadata": {
        "id": "zb1eJbteF4sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "num=0\n",
        "for dirpath, dirnames, filenames in os.walk(str(train_path)):\n",
        "    for filename in [f for f in filenames if f.endswith(\".jpg\")][:1]:\n",
        "        img = PIL.Image.open(str(dirpath)+\"/\"+str(filename))\n",
        "        plt.subplot(3,3,num+1)\n",
        "        plt.title(str(dirpath).split('/')[-1])\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        num=num+1"
      ],
      "metadata": {
        "id": "9Ls9KLOwDxF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "hDibaAZnDxIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model Creation\n",
        "\n",
        "## Model 0\n"
      ],
      "metadata": {
        "id": "I5QWMYswGWx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model\n",
        "model=Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.Rescaling(scale=1./255., offset=0.0,),         \n",
        "    \n",
        "    Conv2D(32,(3,3),input_shape=(img_height,img_width,3),activation='relu',padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.1),\n",
        "    \n",
        "    Conv2D(64,(3,3),activation='relu',padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.1),\n",
        "    \n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.25),   \n",
        "    Dense(9, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "bbaOD_e2JbBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "u9k2Xc5ireLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")\n"
      ],
      "metadata": {
        "id": "f2ylJ9mvreN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the summary of all layers\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Mm7bw9lZreQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing training results\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WB_pirpZreTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "- Training accuracy = 83% \n",
        "- Validation accuracy = 54% \n",
        "- It is not in par with the training accuracy.\n",
        "- The validation loss as observed is very high.\n",
        "- Indicative of some Overfit in the model.\n",
        "- We could add some Dropout layers and remove the BatchNormalization layers.\n",
        "- And by adding a few more layers, we could improve the accuracy by trying to extract more features.\n"
      ],
      "metadata": {
        "id": "A0DnymZ5sKCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1"
      ],
      "metadata": {
        "id": "L2IbaAZWsl71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the Model\n",
        "model_update=Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.Rescaling(scale=1./255., offset=0.0,),         \n",
        "                             \n",
        "    Conv2D(32,(3,3),input_shape=(img_height,img_width,3),activation='relu',padding='same'),\n",
        "    Conv2D(32,(3,3),activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.7),\n",
        "    \n",
        "    Conv2D(64,(3,3),activation='relu',padding='same'),\n",
        "    Conv2D(64,(3,3),activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.7),\n",
        "    \n",
        "    Conv2D(128,(3,3),activation='relu',padding='same'),\n",
        "    Conv2D(128,(3,3),activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.7),\n",
        "    \n",
        "    Flatten(),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.25),   \n",
        "    Dense(9, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "8aO2kR-XreVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "model_update.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics='accuracy')"
      ],
      "metadata": {
        "id": "vrM0eYyjreYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "epochs = 20\n",
        "history = model_update.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "metadata": {
        "id": "FqpkmSQWreaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the results\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PW-OUAofredR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "- Training Accuracy = 41%.\n",
        "- Validation Accuracy = 43%.\n",
        "- This is a much better model compared to the previous model as there seems to be No Overfit."
      ],
      "metadata": {
        "id": "a2k0FvbTs1jO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Data Augmentation"
      ],
      "metadata": {
        "id": "ELXr168ttCxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifying the Augmentation\n",
        "data_augmentation=tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2)\n",
        "])"
      ],
      "metadata": {
        "id": "PtZABw6Tref1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the Augmented Data\n",
        "image, label = next(iter(train_ds))\n",
        "image=np.array(image,np.int32)  \n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "  augmented_image = data_augmentation(image)\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  augmented_image1=np.array(augmented_image[0],np.int32)  \n",
        "  plt.imshow((augmented_image1))\n",
        "  plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "BXmE_IDdreiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model 2"
      ],
      "metadata": {
        "id": "0QffEcNCtRhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Model\n",
        "model_augmented=Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.Rescaling(scale=1./255., offset=0.0,),         \n",
        "\n",
        "    data_augmentation,\n",
        "\n",
        "    Conv2D(32,(3,3),input_shape=(img_height,img_width,3),activation='relu',padding='same'),\n",
        "    Conv2D(32,(3,3),activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.7),\n",
        "    \n",
        "    Conv2D(64,(3,3),activation='relu',padding='same'),\n",
        "    Conv2D(64,(3,3),activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.7),\n",
        "    \n",
        "    Conv2D(128,(3,3),activation='relu',padding='same'),\n",
        "    Conv2D(128,(3,3),activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.7),\n",
        "    \n",
        "    Flatten(),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.25),   \n",
        "    Dense(9, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "DdXsuWojrek7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "model_augmented.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics='accuracy')"
      ],
      "metadata": {
        "id": "NfGF0tC4renj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "epochs = 20\n",
        "history = model_augmented.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "metadata": {
        "id": "qlO4fQSRreqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the results\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WTbVE96eresv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "- Training accuracy = 42%.\n",
        "- Validation accuracy = 44%.\n",
        "- This is a much better model compared to the previous two models as there seems to be No Overfit.\n",
        "- Data Augmentation has improved the model performance."
      ],
      "metadata": {
        "id": "bEhA35Y3tgkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Checking for Class Imbalance"
      ],
      "metadata": {
        "id": "WYS0M5G_toEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in class_names:\n",
        "    directory =train_path+'/'+i+'/'\n",
        "    class_directory = pathlib.Path(directory)\n",
        "    length=len(list(class_directory.glob('*.jpg')))\n",
        "    print(f'{i} has {length} samples.')"
      ],
      "metadata": {
        "id": "IOAhpH4VrevL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The samples of various classes are not in equal proportion.\n",
        "- There is a significant Class Imbalance observed.\n",
        "- The class with the least number of samples is Seborrheic Keratosis with 77.\n",
        "- The class that dominates the data in terms of proportionate number of samples is Pigmented Benign Keratosis with sample size of 462."
      ],
      "metadata": {
        "id": "P8oxkb4LtuE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Using Augmentor for Class Imbalance Treatment"
      ],
      "metadata": {
        "id": "ZCjHHlVjtzlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Augmentor\n",
        "!pip install Augmentor"
      ],
      "metadata": {
        "id": "raL9BPT8rexq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Augmentor\n",
        "path_to_training_dataset=train_path\n",
        "import Augmentor\n",
        "for i in class_names:\n",
        "    p = Augmentor.Pipeline(path_to_training_dataset + '/' + i)\n",
        "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
        "    p.sample(500) ## We are adding 500 samples per class to make sure that none of the classes are sparse."
      ],
      "metadata": {
        "id": "-veyE6PDre0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Augmentor has stored the augmented images in the output sub-directory of each of the sub-directories of skin cancer types.. Lets take a look at total count of augmented images.\n",
        "image_count_train = len(list(data_dir_train.glob('*/output/*.jpg')))\n",
        "print(image_count_train)"
      ],
      "metadata": {
        "id": "5RrsPeS5JbEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_list = [x for x in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]"
      ],
      "metadata": {
        "id": "6YPELuXFJbG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lesion_list_new = [os.path.basename(os.path.dirname(os.path.dirname(y))) for y in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]"
      ],
      "metadata": {
        "id": "PeY8gaomJbJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe_dict_new = dict(zip(path_list, lesion_list_new))"
      ],
      "metadata": {
        "id": "XLow0esKJbMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in class_names:\n",
        "    directory =train_path+'/'+i+'/'\n",
        "    directory_out =train_path+'/'+i+'/output/'\n",
        "    class_directory = pathlib.Path(directory)\n",
        "    class_directory_out = pathlib.Path(directory_out)\n",
        "    length=len(list(class_directory.glob('*.jpg')))\n",
        "    length_out=len(list(class_directory_out.glob('*.jpg')))\n",
        "    length_tot=length+length_out\n",
        "    print(f'{i} has {length_tot} samples.')"
      ],
      "metadata": {
        "id": "iHYAFYK8uGYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations:\n",
        "- The Augmentor has helped decrease the imbalance in class images and that can be viewed from above."
      ],
      "metadata": {
        "id": "hbyAP8xVuJ4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Modelling Augmented Data"
      ],
      "metadata": {
        "id": "4zsEP2THuQcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ],
      "metadata": {
        "id": "Kl-dr71puGfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Train Data Set\n",
        "data_dir_train=train_path\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir_train,\n",
        "  seed=123, label_mode='categorical',\n",
        "  validation_split = 0.2,\n",
        "  subset = 'training',\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "WMN9ro62uGiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Validation Data Set\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir_train,\n",
        "  seed=123, label_mode='categorical',\n",
        "  validation_split = 0.2,\n",
        "  subset = 'validation',\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "id": "IE_sne4WuGk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Model 3\n"
      ],
      "metadata": {
        "id": "XhuMdgTOuaYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Model\n",
        "model_final=Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.Rescaling(scale=1./255., offset=0.0,),         \n",
        "    \n",
        "    Conv2D(32,(3,3),input_shape=(img_height,img_width,3),activation='relu',padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.1),\n",
        "    \n",
        "    Conv2D(64,(3,3),activation='relu',padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.1),\n",
        "    \n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.25),   \n",
        "    Dense(9, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "8fQT1KTyuGna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the Model\n",
        "model_final.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics='accuracy')"
      ],
      "metadata": {
        "id": "KQnPdeccuGqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model\n",
        "epochs = 30\n",
        "## Your code goes here, use 50 epochs.\n",
        "history = model_final.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "metadata": {
        "id": "HGDs7WfhuGs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing the model results\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fkQi998duGvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "- Training accuracy = ~88%.\n",
        "- Validation accuracy = ~78%.\n",
        "- Though the model accuracy has improved, the class rebalance has helped treat the overfitting to some extent.\n",
        "- Much better models could be built or tried out using more epochs and more layers."
      ],
      "metadata": {
        "id": "W3fCulbOutOT"
      }
    }
  ]
}